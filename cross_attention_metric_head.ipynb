{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYtuWHxWiBkAulMNRwr6dO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsanoop887-hash/AIF360/blob/main/cross_attention_metric_head.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aCcxANTQmJ71"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionMetricHead(nn.Module):\n",
        "\n",
        "\n",
        " def __init__(self,d_model=256 ,nhead=4,dim_feedforward=512):\n",
        "  super().__init__()\n",
        "  self.cross_attn=nn.MultiheadAttention(d_model,nhead,batch_first=True)\n",
        "  self.ff=nn.Sequential(\n",
        "      nn.LayerNorm(d_model),\n",
        "      nn.Linear(d_model,dim_feedforward),\n",
        "      nn.GELU(),\n",
        "      nn.Linear(dim_feedforward,d_model)\n",
        "  )\n",
        "  self.score_head=nn.Sequential(\n",
        "      nn.LayerNorm(d_model),\n",
        "      nn.Linear(d_model,d_model//2),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(d_model//2,1)\n",
        "  )\n",
        "\n",
        " def forward(self,prompt,response,prompt_mask=None,response_mask=None):\n",
        "  attn_output, _=self.cross_attn(response,prompt,prompt,\n",
        "                                 key_padding_mask=~prompt_mask if prompt_mask is not None else None)\n",
        "  x=response+attn_output\n",
        "  x=self.ff(x)\n",
        "\n",
        "  if response_mask is not None:\n",
        "    mask=response_mask.unsqueeze(-1).float()\n",
        "    pooled=(x*mask).sum(dim=1)/mask.sum(dim=1).clamp_min(1e-6)\n",
        "  else:\n",
        "    pooled=x.mean(dim=1)\n",
        "\n",
        "  score = self.score_head(pooled).squeeze(-1)\n",
        "  return score"
      ],
      "metadata": {
        "id": "IWtF2bbKmg3g"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CAMT(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, d_model=256, nhead=8, num_layers=2, num_metrics=3):\n",
        "        super().__init__()\n",
        "        encoder_layer=nn.TransformerEncoderLayer(d_model, nhead, 512, batch_first=True)\n",
        "        self.encoder=nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n",
        "        self.pos_enc=nn.Parameter(torch.randn(1,512,d_model))\n",
        "\n",
        "        self.metric_heads=nn.ModuleList([CrossAttentionMetricHead(d_model,nhead//2,512)for _ in range(num_metrics)])\n",
        "\n",
        "  def forward(self, prompt_embeds, response_embeds, prompt_mask=None, response_mask=None):\n",
        "\n",
        "    prompt=prompt_embeds+self.pos_enc[:,:prompt_embeds.size(1),:]\n",
        "    response=response_embeds+self.pos_enc[:,:response_embeds.size(1),:]\n",
        "\n",
        "    prompt_encoded = self.encoder(prompt, src_key_padding_mask=~prompt_mask if prompt_mask is not None else None)\n",
        "    response_encoded = self.encoder(response, src_key_padding_mask=~response_mask if response_mask is not None else None)\n",
        "\n",
        "    scores= []\n",
        "    for head in self.metric_heads:\n",
        "      s=head(prompt_encoded, response_encoded, prompt_mask, response_mask)\n",
        "      scores.append(s)\n",
        "\n",
        "    scores = torch.stack(scores, dim=1);\n",
        "    probs = torch.sigmoid(scores)\n",
        "    return {\n",
        "              \"raw\": scores,\n",
        "              \"prob\": probs,\n",
        "          }"
      ],
      "metadata": {
        "id": "jJaTfcwwqbYg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\" :\n",
        "  torch.manual_seed(0)\n",
        "  B, P, R, D = 2, 10, 15, 256\n",
        "  model = CAMT(d_model=D, nhead=8, num_layers=2, num_metrics=3)\n",
        "  prompt_embeds = torch.randn(B, P, D)\n",
        "  response_embeds = torch.randn(B, R, D)\n",
        "  prompt_mask = torch.ones(B, P, dtype=torch.bool)\n",
        "  response_mask = torch.ones(B, R, dtype=torch.bool)\n",
        "\n",
        "  out = model(prompt_embeds, response_embeds, prompt_mask, response_mask)\n",
        "  print(\"Raw metric scores:\", out[\"raw\"])\n",
        "  print(\"Prob metric scores (0–1):\", out[\"prob\"])\n",
        "  print(\"AGI Index (example):\", (out[\"prob\"] @ torch.tensor([0.5, 0.3, 0.2])).detach().cpu())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db-fkO-1uKfs",
        "outputId": "d1bbd33f-c1fa-421a-cf88-edac806af5fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw metric scores: tensor([[-0.0744,  0.2996, -0.0325],\n",
            "        [-0.0189,  0.3758,  0.0651]], grad_fn=<StackBackward0>)\n",
            "Prob metric scores (0–1): tensor([[0.4814, 0.5744, 0.4919],\n",
            "        [0.4953, 0.5929, 0.5163]], grad_fn=<SigmoidBackward0>)\n",
            "AGI Index (example): tensor([0.5114, 0.5287])\n"
          ]
        }
      ]
    }
  ]
}