{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFYsY/uD36IMBxaCYfiRUo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsanoop887-hash/AIF360/blob/main/alphaone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lCL3GbR9WQdj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Chess4x4:\n",
        "    def __init__(self) :\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.array([\n",
        "            [4,2,3,6],\n",
        "            [1,1,1,1],\n",
        "            [-1,-1,-1,-1],\n",
        "            [-4,-2,-3,-6]\n",
        "        ],dtype=int)\n",
        "        self.current_player=1\n",
        "        return self.board\n",
        "\n",
        "    def in_bounds(self,x,y):\n",
        "        return 0 <= x < 4 and 0 <= y < 4\n",
        "\n",
        "    def get_legal_moves(self):\n",
        "        moves = []\n",
        "        positions = list(zip(*np.where(self.board*self.current_player>0)))\n",
        "        for x,y in positions:\n",
        "            piece = abs(self.board[x,y])\n",
        "            # Pawn\n",
        "            if piece==1:\n",
        "                dx = -1 if self.current_player==1 else 1\n",
        "                if self.in_bounds(x+dx,y) and self.board[x+dx,y]==0:\n",
        "                    moves.append(((x,y),(x+dx,y)))\n",
        "                for dy in [-1,1]:\n",
        "                    if self.in_bounds(x+dx,y+dy) and self.board[x+dx,y+dy]*self.current_player<0:\n",
        "                        moves.append(((x,y),(x+dx,y+dy)))\n",
        "            # Knight\n",
        "            elif piece==2:\n",
        "                for dx,dy in [(-2,-1),(-2,1),(-1,-2),(-1,2),(1,-2),(1,2),(2,-1),(2,1)]:\n",
        "                    nx,ny = x+dx,y+dy\n",
        "                    if self.in_bounds(nx,ny) and self.board[nx,ny]*self.current_player<=0:\n",
        "                        moves.append(((x,y),(nx,ny)))\n",
        "            # Bishop\n",
        "            elif piece==3:\n",
        "                for dx,dy in [(-1,-1),(-1,1),(1,-1),(1,1)]:\n",
        "                    nx,ny = x+dx,y+dy\n",
        "                    while self.in_bounds(nx,ny):\n",
        "                        if self.board[nx,ny]*self.current_player>0: break\n",
        "                        moves.append(((x,y),(nx,ny)))\n",
        "                        if self.board[nx,ny]*self.current_player<0: break\n",
        "                        nx,ny = nx+dx,ny+dy\n",
        "            # Rook\n",
        "            elif piece==4:\n",
        "                for dx,dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
        "                    nx,ny = x+dx,y+dy\n",
        "                    while self.in_bounds(nx,ny):\n",
        "                        if self.board[nx,ny]*self.current_player>0: break\n",
        "                        moves.append(((x,y),(nx,ny)))\n",
        "                        if self.board[nx,ny]*self.current_player<0: break\n",
        "                        nx,ny = nx+dx,ny+dy\n",
        "            # Queen\n",
        "            elif piece==5:\n",
        "                for dx,dy in [(-1,-1),(-1,1),(1,-1),(1,1),(-1,0),(1,0),(0,-1),(0,1)]:\n",
        "                    nx,ny = x+dx,y+dy\n",
        "                    while self.in_bounds(nx,ny):\n",
        "                        if self.board[nx,ny]*self.current_player>0: break\n",
        "                        moves.append(((x,y),(nx,ny)))\n",
        "                        if self.board[nx,ny]*self.current_player<0: break\n",
        "                        nx,ny = nx+dx,ny+dy\n",
        "            # King\n",
        "            elif piece==6:\n",
        "                for dx in [-1,0,1]:\n",
        "                    for dy in [-1,0,1]:\n",
        "                        nx,ny = x+dx,y+dy\n",
        "                        if self.in_bounds(nx,ny) and self.board[nx,ny]*self.current_player<=0 and (dx!=0 or dy!=0):\n",
        "                            moves.append(((x,y),(nx,ny)))\n",
        "        return moves\n",
        "\n",
        "    def make_move(self,move):\n",
        "        (fx,fy),(tx,ty) = move\n",
        "        self.board[tx,ty] = self.board[fx,fy]\n",
        "        self.board[fx,fy] = 0\n",
        "        self.current_player *= -1\n",
        "\n",
        "    def is_game_over(self):\n",
        "        kings = np.where(np.abs(self.board)==6)\n",
        "        return len(kings[0])<2\n",
        "\n",
        "    def get_winner(self):\n",
        "        kings = np.where(np.abs(self.board)==6)\n",
        "        if len(kings[0])<2:\n",
        "            return self.current_player*-1\n",
        "        return 0"
      ],
      "metadata": {
        "id": "4uQr6CcFW-tw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaoneNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 2)\n",
        "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
        "        self.policy_head = nn.Linear(128, 16 * 16)\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        p = torch.softmax(self.policy_head(x), dim=1)\n",
        "        v = torch.tanh(self.value_head(x))\n",
        "        return p, v"
      ],
      "metadata": {
        "id": "Plnb3QU-aL81"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.N = {} # Visit counts\n",
        "        self.W = {} # Total value\n",
        "        self.Q = {} # Mean value\n",
        "        self.P = {} # Prior probability from policy net\n",
        "\n",
        "def mcts(root, net, sims=100, c_puct=1.0):\n",
        "    for _ in range(sims):\n",
        "        node = root\n",
        "        path = []\n",
        "\n",
        "        # Selection\n",
        "        while node.children:\n",
        "            ucb_max = -float('inf')\n",
        "            best_move = None\n",
        "            for move, child in node.children.items():\n",
        "                # Calculate UCB\n",
        "                # Add a small value to the denominator to prevent division by zero if N[move] is 0\n",
        "                u = node.Q[move] + c_puct * node.P.get(move, 0) * np.sqrt(sum(node.N.values()) + 1) / (node.N.get(move, 0) + 1e-8)\n",
        "                if u > ucb_max:\n",
        "                    ucb_max = u\n",
        "                    best_move = move\n",
        "            # If there are children but no best_move found (e.g., due to all moves having infinite negative UCB, which shouldn't happen with this formula, but as a safeguard)\n",
        "            if best_move is None:\n",
        "                # Fallback to selecting a random child if selection fails\n",
        "                if node.children:\n",
        "                    best_move = random.choice(list(node.children.keys()))\n",
        "                else:\n",
        "                    # If no children, break from selection loop\n",
        "                    break\n",
        "\n",
        "\n",
        "            node = node.children[best_move]\n",
        "            path.append((node, best_move))\n",
        "\n",
        "        # Check if game is over at the selected node\n",
        "        if node.state.is_game_over():\n",
        "            value = node.state.get_winner()\n",
        "            # Backpropagate the value\n",
        "            for path_node, move in reversed(path):\n",
        "                # Ensure move exists in path_node.N before accessing\n",
        "                if move in path_node.N:\n",
        "                    path_node.N[move] += 1\n",
        "                    # Value is from the perspective of the player *after* the move\n",
        "                    path_node.W[move] += value * path_node.state.current_player\n",
        "                    path_node.Q[move] = path_node.W[move] / path_node.N[move]\n",
        "            continue # Go to the next simulation\n",
        "\n",
        "        # Expansion and Evaluation\n",
        "        legal_moves = node.state.get_legal_moves()\n",
        "        if legal_moves:\n",
        "            board_tensor = torch.tensor(node.state.board, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                p, v = net(board_tensor)\n",
        "            p = p.squeeze().numpy()\n",
        "\n",
        "            # Initialize child nodes and their stats for all legal moves\n",
        "            move_probs = {}\n",
        "            sum_probs = 0\n",
        "            for move in legal_moves:\n",
        "                # Flatten the move to a single index\n",
        "                from_idx = move[0][0] * 4 + move[0][1]\n",
        "                to_idx = move[1][0] * 4 + move[1][1]\n",
        "                flat_move_idx = from_idx * 16 + to_idx # Assuming 16*16 policy head output\n",
        "\n",
        "                # Get probability from network, handle potential index out of bounds (though 16*16 should cover all 4x4 moves)\n",
        "                prob = p[flat_move_idx] if flat_move_idx < len(p) else 0.0\n",
        "                move_probs[move] = prob\n",
        "                sum_probs += prob\n",
        "\n",
        "                # Create child node if it doesn't exist and initialize stats\n",
        "                if move not in node.children:\n",
        "                    new_state = Chess4x4()\n",
        "                    new_state.board = np.copy(node.state.board) # Deep copy the board\n",
        "                    new_state.current_player = node.state.current_player\n",
        "                    new_state.make_move(move)\n",
        "                    child_node = Node(new_state, parent=node)\n",
        "                    node.children[move] = child_node\n",
        "\n",
        "                    # Initialize child node's stats\n",
        "                    node.N[move] = 0\n",
        "                    node.W[move] = 0\n",
        "                    node.Q[move] = 0\n",
        "                    # Initialize P with a small value or uniform if sum_probs is 0\n",
        "                    node.P[move] = prob if sum_probs > 1e-8 else 1.0 / len(legal_moves)\n",
        "\n",
        "\n",
        "            # Normalize probabilities if sum_probs > 0\n",
        "            if sum_probs > 1e-8:\n",
        "                for move in legal_moves:\n",
        "                    node.P[move] = move_probs[move] / sum_probs\n",
        "            else:\n",
        "                 # If sum of probabilities is zero, distribute uniformly among legal moves\n",
        "                 for move in legal_moves:\n",
        "                    node.P[move] = 1.0 / len(legal_moves)\n",
        "\n",
        "\n",
        "            # Backpropagate the value from the neural network (from the perspective of the current player)\n",
        "            value = v.item()\n",
        "            for path_node, move in reversed(path):\n",
        "                # Ensure move exists in path_node.N before accessing\n",
        "                if move in path_node.N:\n",
        "                    path_node.N[move] += 1\n",
        "                    # Value is from the perspective of the player *after* the move\n",
        "                    path_node.W[move] += value * path_node.state.current_player\n",
        "                    path_node.Q[move] = path_node.W[move] / path_node.N[move]\n",
        "\n",
        "\n",
        "        else: # No legal moves, game might be a draw or the player is checkmated\n",
        "             # In this simplified version, we'll just backpropagate a value of 0 (draw)\n",
        "             value = 0\n",
        "             for path_node, move in reversed(path):\n",
        "                # Ensure move exists in path_node.N before accessing\n",
        "                if move in path_node.N:\n",
        "                    path_node.N[move] += 1\n",
        "                    path_node.W[move] += value * path_node.state.current_player\n",
        "                    path_node.Q[move] = path_node.W[move] / path_node.N[move]\n",
        "\n",
        "    # Return normalized visit counts for training policy\n",
        "    policy = {}\n",
        "    N_total = sum(root.N.values())\n",
        "    if N_total > 0:\n",
        "        for move, n in root.N.items():\n",
        "            policy[move] = n / N_total\n",
        "    # If N_total is 0 (e.g., no simulations completed or root had no legal moves),\n",
        "    # return a uniform policy over legal moves as a fallback.\n",
        "    elif root.state.get_legal_moves():\n",
        "         legal_moves = root.state.get_legal_moves()\n",
        "         for move in legal_moves:\n",
        "             policy[move] = 1.0 / len(legal_moves)\n",
        "    # If no legal moves at all from the root, return empty policy\n",
        "    else:\n",
        "        policy = {}\n",
        "\n",
        "\n",
        "    return policy"
      ],
      "metadata": {
        "id": "HmC-AwRoqME6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  while node.children:\n",
        "            ucb_max = -float('inf')\n",
        "            best_move = None\n",
        "            for move,child in node.children.items():\n",
        "                u = node.Q[move] + c_puct*node.P[move]*np.sqrt(sum(node.N.values())+1)/(1+node.N[move])\n",
        "                if u>ucb_max:\n",
        "                    ucb_max = u\n",
        "                    best_move = move\n",
        "            node = node.children[best_move]\n",
        "            path.append((node,best_move))\n",
        "\n",
        "        # Evaluation\n",
        "        board_tensor = torch.tensor(node.state.board,dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            p,v = net(board_tensor)\n",
        "        p = p.squeeze().numpy()\n",
        "\n",
        "        # Expansion\n",
        "        legal_moves = node.state.get_legal_moves()\n",
        "        for i,move in enumerate(legal_moves):\n",
        "            if move not in node.children:\n",
        "                node.children[move] = Node(state=node.state)\n",
        "                node.P[move] = p[i] if i<len(p) else 1/len(legal_moves)\n",
        "                node.N[move] = 0\n",
        "                node.W[move] = 0\n",
        "                node.Q[move] = 0\n",
        "\n",
        "        # Backup\n",
        "        for parent_node,move in reversed(path):\n",
        "            parent_node.N[move] += 1\n",
        "            parent_node.W[move] += v.item()\n",
        "            parent_node.Q[move] = parent_node.W[move]/parent_node.N[move]\n",
        "\n",
        "    # Return normalized policy for training\n",
        "    N_total = sum(root.N.values())\n",
        "    policy = {move:n/N_total for move,n in root.N.items()}\n",
        "    return policy\n",
        "\n",
        "# ==============================\n",
        "# 4. Self-Play\n",
        "# ==============================\n",
        "def play_game(net, sims=100):\n",
        "    game = Chess4x4()\n",
        "    states, pis, zs = [],[],[]\n",
        "    while not game.is_game_over():\n",
        "        root = Node(game)\n",
        "        policy = mcts(root, net, sims=sims)\n",
        "        if policy:\n",
        "            move = max(policy,key=policy.get)\n",
        "        else:\n",
        "            move = random.choice(game.get_legal_moves())\n",
        "        states.append(game.board.copy())\n",
        "        pis.append([policy.get(m,0) for m in root.children])\n",
        "        game.make_move(move)\n",
        "    winner = game.get_winner()\n",
        "    zs = [winner]*len(states)\n",
        "    return states,pis,zs\n",
        "\n",
        "# ==============================\n",
        "# 5. Training\n",
        "# ==============================\n",
        "def train_network(net, replay_buffer, optimizer, batch_size=64):\n",
        "    batch = random.sample(replay_buffer,min(batch_size,len(replay_buffer)))\n",
        "    state_batch = torch.tensor([b[0] for b in batch],dtype=torch.float32).unsqueeze(1)\n",
        "    policy_batch = torch.tensor([b[1] for b in batch],dtype=torch.float32)\n",
        "    value_batch = torch.tensor([b[2] for b in batch],dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    p_pred,v_pred = net(state_batch)\n",
        "    policy_loss = -(policy_batch*torch.log(p_pred+1e-8)).sum(dim=1).mean()\n",
        "    value_loss = nn.MSELoss()(v_pred,value_batch)\n",
        "    loss = policy_loss + value_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# ==============================\n",
        "# 6. Human vs AI\n",
        "# ==============================\n",
        "def human_vs_ai(net, sims=100):\n",
        "    game = Chess4x4()\n",
        "    while not game.is_game_over():\n",
        "        print(game.board)\n",
        "        if game.current_player==1:\n",
        "            legal = game.get_legal_moves()\n",
        "            for i,m in enumerate(legal):\n",
        "                print(f\"{i}: {m}\")\n",
        "            move_idx = int(input(f\"Your move (0-{len(legal)-1}): \"))\n",
        "            move = legal[move_idx]\n",
        "            game.make_move(move)\n",
        "        else:\n",
        "            root = Node(game)\n",
        "            policy = mcts(root, net, sims=sims)\n",
        "            move = max(policy,key=policy.get)\n",
        "            print(\"AI moves:\", move)\n",
        "            game.make_move(move)\n",
        "    winner = game.get_winner()\n",
        "    print(\"Winner:\", \"Human\" if winner==1 else \"AI\" if winner==-1 else \"Draw\")\n",
        "\n",
        "# ==============================\n",
        "# 7. Main Loop\n",
        "# ==============================\n",
        "if __name__==\"__main__\":\n",
        "    net = AlphaZeroNet()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    replay_buffer = []\n",
        "\n",
        "    # Self-play training\n",
        "    for iteration in range(10):  # increase for stronger AI\n",
        "        states,pis,zs = play_game(net, sims=50)\n",
        "        for s,p,z in zip(states,pis,zs):\n",
        "            replay_buffer.append((s,p,z))\n",
        "        loss = train_network(net,replay_buffer,optimizer)\n",
        "        print(f\"Iteration {iteration}, Loss: {loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "1iflvz5BqgHI",
        "outputId": "1870c417-cb4f-44de-8f4d-0d1e17ce2a59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 13)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    board_tensor = torch.tensor(node.state.board,dtype=torch.float32).unsqueeze(0).unsqueeze(0)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42c9a408"
      },
      "source": [
        "# ==============================\n",
        "# 4. Self-Play\n",
        "# ==============================\n",
        "def play_game(net, sims=100):\n",
        "    game = Chess4x4()\n",
        "    states, pis, zs = [], [], []\n",
        "    while not game.is_game_over():\n",
        "        root = Node(game)\n",
        "        policy = mcts(root, net, sims=sims)\n",
        "        if policy:\n",
        "            # Select move based on visit counts (proportional to policy from MCTS)\n",
        "            moves, visit_counts = zip(*policy.items())\n",
        "            # Add a small epsilon to avoid division by zero if all visit counts are zero\n",
        "            sum_visits = sum(visit_counts) + 1e-8\n",
        "            move_probabilities = np.array(visit_counts) / sum_visits\n",
        "            chosen_move_index = np.random.choice(len(moves), p=move_probabilities)\n",
        "            move = moves[chosen_move_index]\n",
        "\n",
        "        else:\n",
        "            # If MCTS returns no legal moves (shouldn't happen if game.get_legal_moves() is correct)\n",
        "            # Fallback to random move (should ideally not be reached in a correct implementation)\n",
        "            legal_moves = game.get_legal_moves()\n",
        "            if legal_moves:\n",
        "                move = random.choice(legal_moves)\n",
        "            else:\n",
        "                break # Should indicate a draw or stalemate if no legal moves\n",
        "\n",
        "        states.append(game.board.copy())\n",
        "\n",
        "        # Convert policy dictionary to a list of probabilities corresponding to all possible moves (for fixed policy size)\n",
        "        # This assumes the policy head in AlphaoneNet outputs probabilities for all possible 16*16 moves\n",
        "        # We need a consistent ordering of moves to match the policy output\n",
        "        # A simple approach is to map each (from, to) move to a unique index\n",
        "        # from_idx = move[0][0] * 4 + move[0][1]\n",
        "        # to_idx = move[1][0] * 4 + move[1][1]\n",
        "        # flat_move_idx = from_idx * 16 + to_idx\n",
        "\n",
        "        # To get the policy vector matching the neural network output, we need to\n",
        "        # create a vector of size 16*16, where each element corresponds to a\n",
        "        # possible move (from, to) in flattened index form, and populate it\n",
        "        # with the probabilities from the MCTS policy for the legal moves.\n",
        "        # This is complex because the MCTS policy only contains legal moves.\n",
        "\n",
        "        # A simpler approach for now is to store the policy dictionary directly,\n",
        "        # or convert it to a format that can be easily used for training.\n",
        "        # For training, we need a policy vector where illegal moves have zero probability.\n",
        "\n",
        "        # Let's create a policy vector of size 16*16 where legal moves have their probabilities\n",
        "        # and illegal moves have 0.\n",
        "        policy_vector = np.zeros(16 * 16)\n",
        "        total_visits = sum(policy.values()) + 1e-8 # Add epsilon for stability\n",
        "        for m, visits in policy.items():\n",
        "            from_idx = m[0][0] * 4 + m[0][1]\n",
        "            to_idx = m[1][0] * 4 + m[1][1]\n",
        "            flat_move_idx = from_idx * 16 + to_idx\n",
        "            policy_vector[flat_move_idx] = visits / total_visits\n",
        "\n",
        "        pis.append(policy_vector)\n",
        "\n",
        "        game.make_move(move)\n",
        "\n",
        "    winner = game.get_winner()\n",
        "    zs = [winner] * len(states)\n",
        "    return states, pis, zs\n",
        "\n",
        "# ==============================\n",
        "# 5. Training\n",
        "# ==============================\n",
        "def train_network(net, replay_buffer, optimizer, batch_size=64):\n",
        "    if not replay_buffer:\n",
        "        return 0.0  # Return 0 loss if replay buffer is empty\n",
        "\n",
        "    batch = random.sample(replay_buffer, min(batch_size, len(replay_buffer)))\n",
        "\n",
        "    state_batch = torch.tensor([b[0] for b in batch], dtype=torch.float32).unsqueeze(1)\n",
        "    # Ensure policy_batch has the correct shape (batch_size, 16*16)\n",
        "    policy_batch = torch.tensor([b[1] for b in batch], dtype=torch.float32)\n",
        "    value_batch = torch.tensor([b[2] for b in batch], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    p_pred, v_pred = net(state_batch)\n",
        "\n",
        "    # Ensure p_pred and policy_batch have compatible shapes for loss calculation\n",
        "    # If p_pred is (batch_size, 256) and policy_batch is (batch_size, 256), this is fine.\n",
        "    policy_loss = -(policy_batch * torch.log(p_pred + 1e-8)).sum(dim=1).mean()\n",
        "    value_loss = nn.MSELoss()(v_pred, value_batch)\n",
        "    loss = policy_loss + value_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# ==============================\n",
        "# 6. Human vs AI\n",
        "# ==============================\n",
        "def human_vs_ai(net, sims=100):\n",
        "    game = Chess4x4()\n",
        "    while not game.is_game_over():\n",
        "        print(game.board)\n",
        "        if game.current_player == 1:\n",
        "            legal = game.get_legal_moves()\n",
        "            if not legal:\n",
        "                print(\"No legal moves for Human. Game ends.\")\n",
        "                break\n",
        "            for i, m in enumerate(legal):\n",
        "                print(f\"{i}: {m}\")\n",
        "            try:\n",
        "                move_idx = int(input(f\"Your move (0-{len(legal)-1}): \"))\n",
        "                if 0 <= move_idx < len(legal):\n",
        "                    move = legal[move_idx]\n",
        "                    game.make_move(move)\n",
        "                else:\n",
        "                    print(\"Invalid move index. Please try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a number.\")\n",
        "        else:\n",
        "            root = Node(game)\n",
        "            policy = mcts(root, net, sims=sims)\n",
        "            if policy:\n",
        "                # Select the move with the highest visit count\n",
        "                move = max(policy, key=policy.get)\n",
        "                print(\"AI moves:\", move)\n",
        "                game.make_move(move)\n",
        "            else:\n",
        "                print(\"AI has no legal moves. Game ends.\")\n",
        "                break # Should indicate a draw or stalemate\n",
        "\n",
        "    winner = game.get_winner()\n",
        "    if winner == 1:\n",
        "        print(\"Winner: Human\")\n",
        "    elif winner == -1:\n",
        "        print(\"Winner: AI\")\n",
        "    else:\n",
        "        print(\"Draw\")\n",
        "\n",
        "# ==============================\n",
        "# 7. Main Loop\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    net = AlphaoneNet()  # Use AlphaoneNet as defined in the other cell\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    replay_buffer = []\n",
        "\n",
        "    # Self-play training\n",
        "    for iteration in range(10):  # increase for stronger AI\n",
        "        print(f\"Starting iteration {iteration}...\")\n",
        "        states, pis, zs = play_game(net, sims=50)\n",
        "        print(f\"Game finished in iteration {iteration}. Collected {len(states)} states.\")\n",
        "        for s, p, z in zip(states, pis, zs):\n",
        "            replay_buffer.append((s, p, z))\n",
        "        print(f\"Replay buffer size: {len(replay_buffer)}\")\n",
        "        if replay_buffer:\n",
        "            loss = train_network(net, replay_buffer, optimizer)\n",
        "            print(f\"Iteration {iteration}, Loss: {loss}\")\n",
        "        else:\n",
        "            print(f\"Iteration {iteration}, No data in replay buffer to train.\")\n",
        "\n",
        "\n",
        "    # Example of playing against the AI (optional)\n",
        "    # print(\"\\nStarting Human vs AI game...\")\n",
        "    # human_vs_ai(net, sims=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6f178a",
        "outputId": "4431bf5d-3066-4a1c-8ec2-1b2cca1fc31e"
      },
      "source": [
        "game = Chess4x4()\n",
        "print(\"Initial board state:\")\n",
        "print(game.board)\n",
        "print(\"Current player:\", game.current_player)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial board state:\n",
            "[[ 4  2  3  6]\n",
            " [ 1  1  1  1]\n",
            " [-1 -1 -1 -1]\n",
            " [-4 -2 -3 -6]]\n",
            "Current player: 1\n"
          ]
        }
      ]
    }
  ]
}